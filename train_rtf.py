#!/usr/bin/env python
"""
Rectified Flow è®­ç»ƒè„šæœ¬
åœ¨ AE æ½œç©ºé—´è®­ç»ƒ RTFï¼Œä½¿ç”¨ Fire + PyTorch Lightning + OmegaConf
"""

import fire
import pytorch_lightning as pl
from omegaconf import OmegaConf
from pathlib import Path

from models.ae import AESystem
from models.rtf import RTFSystem


def train(
    config_path: str = "config/rtf.yaml",
    ae_checkpoint: str = None,
    data_path: str = None,
    **kwargs
):
    """
    è®­ç»ƒ Rectified Flow
    
    Args:
        config_path: RTF é…ç½®æ–‡ä»¶è·¯å¾„
        ae_checkpoint: é¢„è®­ç»ƒ AE checkpoint è·¯å¾„
        data_path: æ•°æ®è·¯å¾„ï¼ˆå¿«æ·å‚æ•°ï¼‰
        **kwargs: å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    
    Example:
        # Direct æ¨¡å¼ + DiT éª¨å¹²ç½‘ç»œ
        python train_rtf.py \\
            --ae_checkpoint=output/ae/checkpoints/last.ckpt \\
            --data_path=/path/to/temporal_data.h5ad \\
            --model__mode=direct \\
            --model__backbone=dit
        
        # Inversion æ¨¡å¼ + MLP éª¨å¹²ç½‘ç»œ
        python train_rtf.py \\
            --ae_checkpoint=output/ae/checkpoints/last.ckpt \\
            --data_path=/path/to/temporal_data.h5ad \\
            --model__mode=inversion \\
            --model__backbone=mlp
    """
    # åŠ è½½é…ç½®
    cfg = OmegaConf.load(config_path)
    
    # å¤„ç†å¿«æ·å‚æ•°
    if ae_checkpoint is not None:
        cfg.model.ae_checkpoint = ae_checkpoint
    
    if data_path is not None:
        cfg.data.data_path = data_path
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if kwargs:
        # å¤„ç†åµŒå¥—å‚æ•°ï¼ˆå¦‚ model__latent_dimï¼‰
        flat_kwargs = {}
        for key, value in kwargs.items():
            if '__' in key:
                # å°† model__latent_dim è½¬æ¢ä¸º model.latent_dim
                key = key.replace('__', '.')
            flat_kwargs[key] = value
        
        cli_cfg = OmegaConf.from_dotlist([f"{k}={v}" for k, v in flat_kwargs.items()])
        cfg = OmegaConf.merge(cfg, cli_cfg)
    
    # éªŒè¯å¿…è¦å‚æ•°
    if cfg.data.data_path is None:
        raise ValueError(
            "data_path must be provided via config or command line\n"
            "Example: --data_path=/path/to/temporal_data.h5ad"
        )
    
    if cfg.model.ae_checkpoint is None:
        raise ValueError(
            "ae_checkpoint must be provided via config or command line\n"
            "Example: --ae_checkpoint=output/ae/checkpoints/last.ckpt"
        )
    
    # æ£€æŸ¥ checkpoint æ˜¯å¦å­˜åœ¨
    checkpoint_path = Path(cfg.model.ae_checkpoint)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"AE checkpoint not found: {checkpoint_path}")
    
    print("=" * 80)
    print("è®­ç»ƒé…ç½®:")
    print("=" * 80)
    print(OmegaConf.to_yaml(cfg))
    print("=" * 80)
    
    # åŠ è½½é¢„è®­ç»ƒçš„ AE
    print(f"\nåŠ è½½é¢„è®­ç»ƒ AE: {cfg.model.ae_checkpoint}")
    ae_system = AESystem.load_from_checkpoint(cfg.model.ae_checkpoint)
    ae_encoder = ae_system.autoencoder.encoder
    ae_decoder = ae_system.autoencoder.decoder  # ğŸ”§ æ·»åŠ decoder
    
    # è‡ªåŠ¨ä» AE ä¸­è·å– latent_dim å¹¶åŒæ­¥åˆ° RTF é…ç½®
    ae_latent_dim = ae_system.autoencoder.latent_dim
    print(f"AE æ½œç©ºé—´ç»´åº¦: {ae_latent_dim}")
    
    # æ£€æŸ¥å¹¶åŒæ­¥ latent_dim
    if cfg.model.latent_dim is None:
        print(f"è­¦å‘Š: RTF é…ç½®ä¸­ latent_dim ä¸º Noneï¼Œè‡ªåŠ¨è®¾ç½®ä¸º AE çš„ latent_dim={ae_latent_dim}")
        cfg.model.latent_dim = ae_latent_dim
    elif cfg.model.latent_dim != ae_latent_dim:
        print(f"è­¦å‘Š: RTF é…ç½®ä¸­ latent_dim={cfg.model.latent_dim} ä¸ AE çš„ latent_dim={ae_latent_dim} ä¸ä¸€è‡´ï¼")
        print(f"è‡ªåŠ¨è¦†ç›–ä¸º AE çš„ latent_dim={ae_latent_dim}")
        cfg.model.latent_dim = ae_latent_dim
    else:
        print(f"âœ“ RTF å’Œ AE çš„ latent_dim ä¸€è‡´: {cfg.model.latent_dim}")
    
    # å†»ç»“ AE Encoder å’Œ Decoder
    if cfg.model.freeze_ae:
        ae_encoder.eval()
        ae_decoder.eval()  # ğŸ”§ åŒæ—¶å†»ç»“decoder
        for param in ae_encoder.parameters():
            param.requires_grad = False
        for param in ae_decoder.parameters():
            param.requires_grad = False
        print("AE Encoder å’Œ Decoder å·²å†»ç»“")
    else:
        print("Warning: AE Encoder å’Œ Decoder æœªå†»ç»“ï¼Œå°†å‚ä¸è®­ç»ƒ")
    
    # åˆ›å»º RTF ç³»ç»Ÿ
    system = RTFSystem(cfg, ae_encoder=ae_encoder, ae_decoder=ae_decoder)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    rtf_params = sum(p.numel() for p in system.model.parameters() if p.requires_grad)
    ae_encoder_params = sum(p.numel() for p in ae_encoder.parameters())
    ae_decoder_params = sum(p.numel() for p in ae_decoder.parameters())
    print(f"\nRTF æ¨¡å‹å‚æ•°æ•°é‡: {rtf_params:,} ({rtf_params / 1e6:.2f}M)")
    print(f"AE Encoder å‚æ•°æ•°é‡: {ae_encoder_params:,} ({ae_encoder_params / 1e6:.2f}M)")
    print(f"AE Decoder å‚æ•°æ•°é‡: {ae_decoder_params:,} ({ae_decoder_params / 1e6:.2f}M)")
    print(f"æ¨¡å¼: {cfg.model.mode}")
    print(f"éª¨å¹²ç½‘ç»œ: {cfg.model.backbone}")
    print(f"æ½œç©ºé—´ç»´åº¦: {cfg.model.latent_dim}")
    print(f"é‡‡æ ·æ­¥æ•°: {cfg.training.sample_steps}")
    print(f"CFG å¼ºåº¦: {cfg.training.cfg_scale}\n")
    
    # åˆ›å»º Trainer
    trainer = pl.Trainer(
        accelerator=cfg.accelerator.accelerator,
        devices=cfg.accelerator.devices,
        precision=cfg.accelerator.precision,
        max_epochs=cfg.training.max_epochs,
        log_every_n_steps=cfg.logging.log_every_n_steps,
        enable_checkpointing=True,
        default_root_dir=cfg.logging.output_dir,
        gradient_clip_val=cfg.accelerator.gradient_clip_val,
        callbacks=[
            pl.callbacks.ModelCheckpoint(
                dirpath=Path(cfg.logging.output_dir) / "checkpoints",
                filename=f"rtf-{cfg.model.mode}-{cfg.model.backbone}-{{epoch:02d}}-{{train_loss:.4f}}",
                monitor="train_loss",
                mode="min",
                save_top_k=3,
                save_last=True,
                every_n_epochs=5,  # æ¯ 5 ä¸ª epoch ä¿å­˜ä¸€æ¬¡
            ),
            pl.callbacks.LearningRateMonitor(logging_interval='epoch'),
            pl.callbacks.RichProgressBar(),  # ä½¿ç”¨å¯Œè¿›åº¦æ¡
        ],
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"å¼€å§‹è®­ç»ƒ Rectified Flow ({cfg.model.mode} æ¨¡å¼ + {cfg.model.backbone} éª¨å¹²ç½‘ç»œ)...")
    trainer.fit(system)
    
    # è®­ç»ƒå®Œæˆ
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {trainer.checkpoint_callback.best_model_path}")
    print(f"æœ€ä½³è®­ç»ƒæŸå¤±: {trainer.checkpoint_callback.best_model_score:.6f}")
    
    # é‡‡æ ·ç›®å½•
    sample_dir = Path(cfg.logging.output_dir) / "samples"
    if sample_dir.exists():
        print(f"é‡‡æ ·ç»“æœä¿å­˜åœ¨: {sample_dir}")
    
    return system, trainer


if __name__ == "__main__":
    fire.Fire(train)

