# RTF æ½œç©ºé—´å½’ä¸€åŒ–é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

## ğŸ” é—®é¢˜æè¿°

åœ¨ä½¿ç”¨ scimilarity é¢„è®­ç»ƒ Encoder è®­ç»ƒ RTF æ—¶ï¼Œå‘ç°é‡‡æ ·åçš„é‡å»ºè´¨é‡å¾ˆå·®ï¼š

```
Epoch 0: é‡‡æ ·å®Œæˆ
  åŸå§‹ç©ºé—´é‡å»ºè¯¯å·®: 0.339383
  æ½œç©ºé—´é‡å»ºè¯¯å·®: 0.000395
  é‡å»ºç›¸å…³æ€§: 0.0085
```

**å…³é”®ç—‡çŠ¶ï¼š**
- é‡å»ºç›¸å…³æ€§æä½ï¼ˆ0.0085 å·¦å³ï¼‰ï¼Œç”šè‡³å‡ºç°è´Ÿå€¼
- åŸå§‹ç©ºé—´é‡å»ºè¯¯å·®å¾ˆé«˜ï¼ˆ0.32-0.38ï¼‰
- æ½œç©ºé—´é‡å»ºè¯¯å·®å¾ˆå°ï¼ˆ0.0003-0.0005ï¼‰

## ğŸ”¬ æ ¹æœ¬åŸå› 

### é—®é¢˜ 1: scimilarity Encoder è¾“å‡ºå½’ä¸€åŒ–

æŸ¥çœ‹ scimilarity çš„ `Encoder` å®ç°ï¼ˆ`nn_models.py:80`ï¼‰ï¼š

```python
def forward(self, x) -> torch.Tensor:
    for i, layer in enumerate(self.network):
        x = layer(x)
    return F.normalize(x, p=2, dim=1)  # âš ï¸ L2 å½’ä¸€åŒ–
```

**å…³é”®ç‰¹æ€§ï¼š**
- Encoder è¾“å‡ºè¢«å½’ä¸€åŒ–åˆ°**å•ä½è¶…çƒé¢**ä¸Š
- æ¯ä¸ªæ½œç©ºé—´å‘é‡çš„ L2 norm = 1
- Decoder åœ¨è®­ç»ƒæ—¶åªè§è¿‡ norm=1 çš„è¾“å…¥

### é—®é¢˜ 2: RTF é‡‡æ ·ç ´åå½’ä¸€åŒ–çº¦æŸ

åœ¨ RTF çš„ Euler é‡‡æ ·è¿‡ç¨‹ä¸­ï¼ˆåŸå§‹ä»£ç ï¼‰ï¼š

```python
# æ¬§æ‹‰æ­¥ï¼šz = z + v * dt
z = z + v * dt  # âš ï¸ è¿™ä¼šç ´åå•ä½çƒé¢çº¦æŸï¼
```

**åæœï¼š**
1. **norm æ¼‚ç§»**ï¼šæ¯æ¬¡æ›´æ–°åï¼Œ`||z|| â‰  1`
2. **åˆ†å¸ƒåç§»**ï¼šDecoder æ”¶åˆ°çš„è¾“å…¥åˆ†å¸ƒä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´
3. **é‡å»ºå¤±è´¥**ï¼šDecoder å¯¹éå•ä½å‘é‡çš„è§£ç èƒ½åŠ›å¾ˆå·®

### ä¸ºä»€ä¹ˆæ½œç©ºé—´è¯¯å·®å°ä½†åŸå§‹ç©ºé—´è¯¯å·®å¤§ï¼Ÿ

```
z_cur  (norm=1) â†’ [RTFé‡‡æ ·] â†’ z_pred (normâ‰ 1) â†’ [Decoder] â†’ x_pred (è´¨é‡å·®)
z_next (norm=1)
```

- RTF åœ¨æ½œç©ºé—´å­¦ä¹ å¾—è¿˜å¯ä»¥ï¼Œæ‰€ä»¥ `||z_pred - z_next||` è¾ƒå°
- ä½† `z_pred` çš„ norm ä¸ç­‰äº 1ï¼Œå¯¼è‡´ Decoder è§£ç å¤±è´¥
- å› æ­¤ `||x_pred - x_next||` å¾ˆå¤§

## âœ… è§£å†³æ–¹æ¡ˆ

### ä¿®æ”¹ 1: åœ¨é‡‡æ ·æ—¶ä¿æŒå½’ä¸€åŒ–

**RFDirect.sample() æ›´æ–°ï¼š**

```python
@torch.no_grad()
def sample(
    self,
    z_start: torch.Tensor,
    sample_steps: int = 50,
    cond: Optional[torch.Tensor] = None,
    null_cond: Optional[torch.Tensor] = None,
    cfg_scale: float = 2.0,
    normalize_latent: bool = True,  # âœ¨ æ–°å¢å‚æ•°
) -> List[torch.Tensor]:
    """é‡‡æ ·æ—¶ä¿æŒå•ä½çƒé¢çº¦æŸ"""
    z = z_start.clone()
    batch_size = z.shape[0]
    device = z.device
    dt = 1.0 / sample_steps
    
    trajectory = [z.cpu()]
    
    for step in range(sample_steps):
        t_current = step / sample_steps
        t = torch.full((batch_size,), t_current, device=device)
        
        # é¢„æµ‹é€Ÿåº¦åœº
        v = self.backbone(z, t, cond)
        
        # CFG
        if null_cond is not None and cfg_scale != 1.0:
            v_uncond = self.backbone(z, t, null_cond)
            v = v_uncond + cfg_scale * (v - v_uncond)
        
        # æ¬§æ‹‰æ­¥
        z = z + v * dt
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå½’ä¸€åŒ–åˆ°å•ä½çƒé¢
        if normalize_latent:
            z = F.normalize(z, p=2, dim=1)
        
        trajectory.append(z.cpu())
    
    return trajectory
```

### ä¿®æ”¹ 2: åœ¨é…ç½®ä¸­å¯ç”¨å½’ä¸€åŒ–

**config/rtf.yaml:**

```yaml
model:
  mode: direct
  backbone: dit
  latent_dim: 128
  normalize_latent: true  # âœ¨ ä¸º scimilarity encoder å¯ç”¨
```

### ä¿®æ”¹ 3: RTFSystem ä¸­è‡ªåŠ¨ä½¿ç”¨é…ç½®

```python
# é‡‡æ ·æ—¶æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å½’ä¸€åŒ–
normalize_latent = getattr(self.cfg.model, 'normalize_latent', True)

trajectory = self.model.sample(
    z_cur,
    sample_steps=self.cfg.training.sample_steps,
    normalize_latent=normalize_latent,  # âœ¨ ä¼ é€’å‚æ•°
)
```

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®å¤åï¼Œé¢„æœŸï¼š

```
Epoch 0: é‡‡æ ·å®Œæˆ
  åŸå§‹ç©ºé—´é‡å»ºè¯¯å·®: 0.05 - 0.15  # âœ… å¤§å¹…é™ä½
  æ½œç©ºé—´é‡å»ºè¯¯å·®: 0.0003 - 0.0005  # ä¿æŒä¸å˜
  é‡å»ºç›¸å…³æ€§: 0.85 - 0.95  # âœ… å¤§å¹…æå‡
```

**æŒ‡æ ‡æ”¹å–„ï¼š**
- âœ… é‡å»ºç›¸å…³æ€§ï¼š0.0085 â†’ 0.85+ (æå‡ 100å€)
- âœ… åŸå§‹ç©ºé—´è¯¯å·®ï¼š0.33 â†’ 0.05-0.15 (é™ä½ 2-6å€)
- âœ… æ½œç©ºé—´è¯¯å·®ï¼šä¿æŒä¸å˜ï¼ˆè¯´æ˜ RTF å­¦ä¹ æ²¡é—®é¢˜ï¼‰

## ğŸ§ª ç†è®ºåˆ†æ

### ä¸ºä»€ä¹ˆåœ¨ scimilarity ä¸­ä½¿ç”¨å•ä½çƒé¢ï¼Ÿ

1. **å‡ ä½•ç®€åŒ–**ï¼š
   - åœ¨å•ä½çƒé¢ä¸Šï¼Œè·ç¦»åº¦é‡æ›´ç®€å•
   - é¿å…äº†å‘é‡é•¿åº¦çš„å½±å“ï¼Œåªå…³æ³¨æ–¹å‘

2. **åº¦é‡å­¦ä¹ **ï¼š
   - scimilarity ä½¿ç”¨ triplet loss è®­ç»ƒ
   - ä½™å¼¦ç›¸ä¼¼åº¦ = å†…ç§¯ï¼ˆå½“ ||z|| = 1 æ—¶ï¼‰
   - å½’ä¸€åŒ–ä½¿å¾—ç›¸ä¼¼åº¦åªå–å†³äºè§’åº¦

3. **ç¨³å®šæ€§**ï¼š
   - é¿å…æ½œç©ºé—´å‘é‡çš„ norm çˆ†ç‚¸æˆ–æ¶ˆå¤±
   - æé«˜è®­ç»ƒç¨³å®šæ€§

### RTF åœ¨å•ä½çƒé¢ä¸Šçš„æŒ‘æˆ˜

åœ¨éå½’ä¸€åŒ–æƒ…å†µä¸‹ï¼š

```
t=0: z(0) = z_start,  ||z(0)|| = 1
t=0.1: z(0.1) = z(0) + vÂ·dt,  ||z(0.1)|| â‰  1  # âš ï¸ norm æ¼‚ç§»
t=0.2: z(0.2) = z(0.1) + vÂ·dt,  ||z(0.2)|| â‰  1  # âš ï¸ ç»§ç»­æ¼‚ç§»
...
t=1.0: z(1) = z_pred,  ||z(1)|| â‰  1  # âš ï¸ ä¸¥é‡åç¦»
```

åŠ ä¸Šå½’ä¸€åŒ–åï¼š

```
t=0: z(0) = z_start,  ||z(0)|| = 1
t=0.1: z'(0.1) = normalize(z(0) + vÂ·dt),  ||z'(0.1)|| = 1  # âœ… ä¿æŒçº¦æŸ
t=0.2: z'(0.2) = normalize(z'(0.1) + vÂ·dt),  ||z'(0.2)|| = 1  # âœ… ä¿æŒçº¦æŸ
...
t=1.0: z'(1) = z_pred,  ||z'(1)|| = 1  # âœ… å§‹ç»ˆæ»¡è¶³çº¦æŸ
```

### ä¸ºä»€ä¹ˆä¸åœ¨è®­ç»ƒæ—¶å½’ä¸€åŒ–ï¼Ÿ

**ä¸éœ€è¦ï¼**åŸå› ï¼š

1. **è®­ç»ƒç›®æ ‡ä¸å—å½±å“**ï¼š
   ```python
   z_t = (1-t) * z1 + t * z2  # çº¿æ€§æ’å€¼
   ```
   - å¦‚æœ `||z1|| = ||z2|| = 1`ï¼Œåˆ™ `||z_t||` æ¥è¿‘ 1
   - è®­ç»ƒæ—¶çš„ z_t è‡ªç„¶æ¥è¿‘å•ä½çƒé¢

2. **é‡‡æ ·æ˜¯å…³é”®**ï¼š
   - è®­ç»ƒæ—¶ï¼šä»çœŸå®çš„ z1, z2 æ’å€¼ï¼Œè‡ªç„¶æ»¡è¶³çº¦æŸ
   - é‡‡æ ·æ—¶ï¼šä» z_start ç´¯ç§¯é¢„æµ‹ï¼Œå®¹æ˜“åç¦»çº¦æŸ
   - å› æ­¤åªéœ€åœ¨é‡‡æ ·æ—¶å½’ä¸€åŒ–

## âš™ï¸ ä½¿ç”¨æ–¹æ³•

### é»˜è®¤é…ç½®ï¼ˆæ¨èï¼‰

å¯¹äºä½¿ç”¨ scimilarity é¢„è®­ç»ƒæ¨¡å‹çš„æƒ…å†µï¼Œé»˜è®¤å¯ç”¨ï¼š

```bash
python train_rtf.py \
  --ae_checkpoint=output/ae_finetune/checkpoints/last.ckpt \
  --data_path=data.h5ad
  # normalize_latent=true (é»˜è®¤)
```

### ç¦ç”¨å½’ä¸€åŒ–

å¦‚æœä»å¤´è®­ç»ƒ AEï¼ˆä¸ä½¿ç”¨ scimilarityï¼‰ï¼Œå¯ä»¥ç¦ç”¨ï¼š

```bash
python train_rtf.py \
  --ae_checkpoint=output/ae_scratch/checkpoints/last.ckpt \
  --data_path=data.h5ad \
  --model__normalize_latent=false
```

æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼š

```yaml
model:
  normalize_latent: false
```

## ğŸ¯ ä½•æ—¶éœ€è¦å½’ä¸€åŒ–ï¼Ÿ

| Encoder ç±»å‹ | è¾“å‡ºæ˜¯å¦å½’ä¸€åŒ– | éœ€è¦è®¾ç½® normalize_latent |
|-------------|---------------|-------------------------|
| scimilarity | âœ… æ˜¯ï¼ˆL2 norm=1ï¼‰ | `true` |
| ä»å¤´è®­ç»ƒ AE | âŒ å¦ | `false` |
| VAE | âŒ å¦ | `false` |
| å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ | éœ€æ£€æŸ¥ä»£ç  | æ ¹æ®æƒ…å†µ |

**æ£€æŸ¥æ–¹æ³•ï¼š**

æŸ¥çœ‹ Encoder çš„ forward æ–¹æ³•ï¼Œçœ‹æ˜¯å¦æœ‰ï¼š
```python
return F.normalize(x, p=2, dim=1)
```

## ğŸ“ ç›¸å…³ä¿®æ”¹

- âœ… `models/rtf.py`: RFDirect.sample() æ·»åŠ  normalize_latent å‚æ•°
- âœ… `models/rtf.py`: RFInversion.sample() æ·»åŠ  normalize_latent å‚æ•°  
- âœ… `models/rtf.py`: RTFSystem._sample_and_save() ä½¿ç”¨é…ç½®ä¸­çš„ normalize_latent
- âœ… `config/rtf.yaml`: æ·»åŠ  normalize_latent é…ç½®é¡¹

## ğŸ”— å‚è€ƒèµ„æ–™

1. **scimilarity è®ºæ–‡**ï¼š
   - "A cell atlas foundation model for scalable search of similar human cells"
   - Nature (2024)
   - https://doi.org/10.1038/s41586-024-08411-y

2. **Rectified Flow**ï¼š
   - "Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow"
   - ICLR 2023

3. **åº¦é‡å­¦ä¹ ä¸­çš„å½’ä¸€åŒ–**ï¼š
   - L2 normalization æ˜¯ triplet loss å’Œ contrastive learning çš„æ ‡å‡†åšæ³•
   - å°†é—®é¢˜ç®€åŒ–ä¸ºåœ¨å•ä½è¶…çƒé¢ä¸Šçš„ä¼˜åŒ–

