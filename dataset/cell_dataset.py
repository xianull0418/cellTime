"""
å•ç»†èƒæ•°æ®é›†
æ”¯æŒé™æ€å’Œæ—¶åºå•ç»†èƒæ•°æ®åŠ è½½
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
import scanpy as sc
from pathlib import Path
from typing import Union, Optional

try:
    import anndata as ad
except ImportError:
    ad = None


def preprocess_counts(
    adata: "ad.AnnData",
    *,
    target_sum: float = 1e4,
    log1p: bool = True,
    copy: bool = False,
    verbose: bool = False,
) -> "ad.AnnData":
    """
    é¢„å¤„ç†åŸå§‹è®¡æ•°æ•°æ®ï¼ˆå‚è€ƒ scimilarity æµç¨‹ï¼‰
    
    æµç¨‹ï¼š
      1. å½’ä¸€åŒ–æ¯ä¸ªç»†èƒçš„æ€»è®¡æ•°åˆ° target_sumï¼ˆé»˜è®¤ 10,000ï¼‰
      2. log(1 + x) å˜æ¢
    
    Args:
        adata: AnnData å¯¹è±¡
        target_sum: å½’ä¸€åŒ–ç›®æ ‡æ€»è®¡æ•°
        log1p: æ˜¯å¦è¿›è¡Œ log1p å˜æ¢
        copy: æ˜¯å¦å¤åˆ¶æ•°æ®
        verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
    
    Returns:
        é¢„å¤„ç†åçš„ AnnData å¯¹è±¡
    """
    if copy:
        adata = adata.copy()
    
    # ğŸ”§ æ£€æŸ¥NaN/Infï¼ˆåœ¨ä»»ä½•å¤„ç†ä¹‹å‰ï¼‰
    import numpy as np
    if hasattr(adata.X, 'toarray'):
        X_check = adata.X.toarray()
    else:
        X_check = adata.X
    
    if np.isnan(X_check).any():
        raise ValueError("æ•°æ®ä¸­åŒ…å« NaN å€¼ï¼Œè¯·å…ˆæ¸…ç†æ•°æ®")
    if np.isinf(X_check).any():
        raise ValueError("æ•°æ®ä¸­åŒ…å« Inf å€¼ï¼Œè¯·å…ˆæ¸…ç†æ•°æ®")
    
    # å¦‚æœæœ‰ layers["counts"]ï¼Œä½¿ç”¨åŸå§‹è®¡æ•°
    if "counts" in adata.layers:
        if verbose:
            print("ä½¿ç”¨ layers['counts'] ä½œä¸ºåŸå§‹è®¡æ•°")
        adata.X = adata.layers["counts"].copy()
    
    # æ£€æŸ¥æ•°æ®èŒƒå›´
    if hasattr(adata.X, 'max'):
        max_val = adata.X.max()
        min_val = adata.X.min()
    else:
        max_val = adata.X.data.max() if hasattr(adata.X, 'data') else float('inf')
        min_val = adata.X.data.min() if hasattr(adata.X, 'data') else 0.0
    
    if verbose:
        print(f"æ•°æ®èŒƒå›´æ£€æŸ¥ï¼šmin={min_val:.4f}, max={max_val:.4f}")
    
    # ğŸ”§ å…³é”®åˆ¤æ–­ï¼šå¦‚æœæœ‰è´Ÿå€¼ï¼Œæ•°æ®å·²ç»è¿‡æŸç§å˜æ¢ï¼Œä¸åº”è¯¥å†é¢„å¤„ç†
    if min_val < 0:
        print(f"âš ï¸  æ£€æµ‹åˆ°è´Ÿå€¼ï¼ˆmin={min_val:.4f}ï¼‰ï¼")
        print(f"æ•°æ®å¯èƒ½å·²ç»è¿‡æ ‡å‡†åŒ–/å½’ä¸€åŒ–å¤„ç†ï¼Œè·³è¿‡é¢„å¤„ç†")
        print(f"å¦‚æœéœ€è¦å¼ºåˆ¶é¢„å¤„ç†ï¼Œè¯·å…ˆå°†æ•°æ®è½¬æ¢ä¸ºåŸå§‹è®¡æ•°")
        return adata
    
    # ğŸ”§ æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦é¢„å¤„ç†
    already_processed = False
    
    # æƒ…å†µ1ï¼šæ•°æ®èŒƒå›´åœ¨[0, 15]ï¼Œå¯èƒ½å·²ç»log1p
    if max_val < 15 and min_val >= 0:
        already_processed = True
        if verbose:
            print(f"æ£€æµ‹åˆ°æ•°æ®å¯èƒ½å·²ç» log1p å˜æ¢ï¼ˆmax={max_val:.2f} < 15ï¼‰")
            print(f"è·³è¿‡é¢„å¤„ç†")
    
    # æƒ…å†µ2ï¼šæœ‰äº›ç»†èƒæ€»è®¡æ•°ä¸º0ï¼ˆä¼šåœ¨normalizeæ—¶äº§ç”Ÿè­¦å‘Šï¼‰
    # è¿™ç§æƒ…å†µä»ç„¶éœ€è¦é¢„å¤„ç†ï¼Œä½†è¦å°å¿ƒå¤„ç†
    
    # ğŸ”§ æ‰§è¡Œé¢„å¤„ç†
    if not already_processed:
        if verbose:
            print(f"æ‰§è¡Œå½’ä¸€åŒ–ï¼štarget_sum={target_sum}")
        sc.pp.normalize_total(adata, target_sum=target_sum)
        
        # log1p å˜æ¢
        if log1p:
            if verbose:
                print("æ‰§è¡Œ log1p å˜æ¢")
            sc.pp.log1p(adata)
            # æ¸…ç† uns ä¸­çš„ log1p æ ‡è®°
            if 'log1p' in adata.uns:
                del adata.uns['log1p']
    else:
        if verbose:
            print("è·³è¿‡é¢„å¤„ç†ï¼ˆæ•°æ®ä¼¼ä¹å·²å¤„ç†ï¼‰")
    
    # ğŸ”§ æœ€ç»ˆéªŒè¯ï¼šæ£€æŸ¥å¤„ç†åæ˜¯å¦æœ‰NaN
    if hasattr(adata.X, 'toarray'):
        X_final = adata.X.toarray()
    else:
        X_final = adata.X
    
    if np.isnan(X_final).any():
        raise ValueError("é¢„å¤„ç†åæ•°æ®åŒ…å« NaNï¼è¯·æ£€æŸ¥åŸå§‹æ•°æ®")
    
    if verbose:
        final_max = adata.X.max() if hasattr(adata.X, 'max') else adata.X.data.max()
        final_min = adata.X.min() if hasattr(adata.X, 'min') else adata.X.data.min()
        print(f"é¢„å¤„ç†å®Œæˆï¼šmin={final_min:.4f}, max={final_max:.4f}")
    
    return adata


def load_anndata(
    data: Union[str, Path, "ad.AnnData"],
    *,
    index_col: int = 0,
    preprocess: bool = True,
    target_sum: float = 1e4,
    verbose: bool = False,
) -> sc.AnnData:
    """
    åŠ è½½ AnnData å¯¹è±¡å¹¶è¿›è¡Œé¢„å¤„ç†
    
    æ”¯æŒæ ¼å¼ï¼š
      - .h5ad, .h5, .hdf5ï¼ˆscanpy è¯»å–ï¼‰
      - .csvï¼ˆç¬¬ä¸€åˆ—ä¸ºç»†èƒ idï¼‰
      - .xlsx/.xlsï¼ˆç¬¬ä¸€åˆ—ä¸ºç»†èƒ idï¼‰
      - å·²ç»æ˜¯ AnnData çš„å¯¹è±¡ï¼ˆç›´æ¥è¿”å›ï¼‰
    
    é¢„å¤„ç†æµç¨‹ï¼ˆå‚è€ƒ scimilarityï¼‰ï¼š
      1. å½’ä¸€åŒ–æ¯ä¸ªç»†èƒçš„æ€»è®¡æ•°åˆ° target_sumï¼ˆé»˜è®¤ 10,000ï¼‰
      2. log(1 + x) å˜æ¢
    
    Args:
        data: æ•°æ®è·¯å¾„æˆ– AnnData å¯¹è±¡
        index_col: CSV/Excel æ–‡ä»¶çš„ç´¢å¼•åˆ—
        preprocess: æ˜¯å¦è¿›è¡Œé¢„å¤„ç†ï¼ˆå½’ä¸€åŒ– + log1pï¼‰
        target_sum: å½’ä¸€åŒ–ç›®æ ‡æ€»è®¡æ•°
        verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
    
    Returns:
        AnnData å¯¹è±¡
    """
    # å·²ç»æ˜¯ AnnData å¯¹è±¡
    if ad is not None and isinstance(data, ad.AnnData):
        adata = data
    else:
        # æ–‡ä»¶è·¯å¾„
        path = Path(data)
        if not path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        if verbose:
            print(f"ä»æ–‡ä»¶åŠ è½½æ•°æ®: {path}")
        
        suffix = path.suffix.lower()
        
        if suffix == ".h5ad":
            adata = sc.read_h5ad(path)
        elif suffix in {".h5", ".hdf5"}:
            adata = sc.read(path)
        elif suffix in {".csv", ".xlsx", ".xls"}:
            if suffix == ".csv":
                df = pd.read_csv(path, index_col=index_col)
            else:
                df = pd.read_excel(path, index_col=index_col)
            
            adata = sc.AnnData(
                X=df.to_numpy(),
                obs=pd.DataFrame(index=df.index),
                var=pd.DataFrame(index=df.columns),
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
        
        if verbose:
            print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œç»´åº¦: {adata.shape}")
    
    # é¢„å¤„ç†
    if preprocess:
        adata = preprocess_counts(adata, target_sum=target_sum, verbose=verbose)
    
    return adata


class StaticCellDataset(Dataset):
    """
    é™æ€å•ç»†èƒæ•°æ®é›†ï¼ˆç”¨äº Autoencoder è®­ç»ƒï¼‰
    æ¯æ¬¡è¿”å›å•ä¸ªç»†èƒçš„è¡¨è¾¾å‘é‡
    """
    
    def __init__(
        self,
        data: Union[str, Path, "ad.AnnData"],
        *,
        index_col: int = 0,
        preprocess: bool = True,
        target_sum: float = 1e4,
        verbose: bool = False,
        seed: Optional[int] = None,
    ):
        """
        Args:
            data: æ•°æ®è·¯å¾„æˆ– AnnData å¯¹è±¡
            index_col: CSV/Excel æ–‡ä»¶çš„ç´¢å¼•åˆ—
            preprocess: æ˜¯å¦è¿›è¡Œé¢„å¤„ç†ï¼ˆå½’ä¸€åŒ– + log1pï¼‰
            target_sum: å½’ä¸€åŒ–ç›®æ ‡æ€»è®¡æ•°ï¼ˆé»˜è®¤ 10,000ï¼Œå‚è€ƒ scimilarityï¼‰
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
            seed: éšæœºç§å­
        """
        if isinstance(data, (str, Path)):
            self.adata = load_anndata(
                data,
                index_col=index_col,
                preprocess=preprocess,
                target_sum=target_sum,
                verbose=verbose
            )
        else:
            self.adata = data
            # å¦‚æœä¼ å…¥çš„æ˜¯ AnnData å¯¹è±¡ï¼Œä¹Ÿè¿›è¡Œé¢„å¤„ç†
            if preprocess:
                self.adata = preprocess_counts(
                    self.adata,
                    target_sum=target_sum,
                    verbose=verbose
                )
        
        self.n_cells, self.n_genes = self.adata.shape
        
        # å°†è¡¨è¾¾çŸ©é˜µè½¬æ¢ä¸º tensorï¼ˆæ”¯æŒç¨€ç–çŸ©é˜µï¼‰
        X = self.adata.X.toarray() if hasattr(self.adata.X, "toarray") else self.adata.X
        self.expressions = torch.tensor(X, dtype=torch.float32)
        
        # éšæœºæ•°ç”Ÿæˆå™¨
        self.rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()
        
        if verbose:
            print(f"StaticCellDataset åˆå§‹åŒ–å®Œæˆ:")
            print(f"  - ç»†èƒæ•°: {self.n_cells}")
            print(f"  - åŸºå› æ•°: {self.n_genes}")
    
    def __len__(self) -> int:
        return self.n_cells
    
    def __getitem__(self, idx: int):
        """
        è¿”å›å•ä¸ªç»†èƒçš„è¡¨è¾¾å‘é‡
        
        Returns:
            x: [n_genes] åŸºå› è¡¨è¾¾å‘é‡
        """
        # éšæœºé‡‡æ ·ï¼ˆå¿½ç•¥ idxï¼‰
        rand_idx = int(self.rng.integers(0, self.n_cells))
        x = self.expressions[rand_idx]
        return x


class TemporalCellDataset(Dataset):
    """
    æ—¶åºå•ç»†èƒæ•°æ®é›†ï¼ˆç”¨äº Rectified Flow è®­ç»ƒï¼‰
    æ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªç»†èƒåŠå…¶ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„ç»†èƒ
    """
    
    def __init__(
        self,
        data: Union[str, Path, "ad.AnnData"],
        *,
        valid_pairs_only: bool = True,
        time_col: str = "time",
        next_cell_col: str = "next_cell_id",
        index_col: int = 0,
        preprocess: bool = True,
        target_sum: float = 1e4,
        verbose: bool = False,
    ):
        """
        Args:
            data: æ•°æ®è·¯å¾„æˆ– AnnData å¯¹è±¡
            valid_pairs_only: æ˜¯å¦åªä½¿ç”¨æœ‰æ•ˆçš„æ—¶åºå¯¹
            time_col: æ—¶é—´åˆ—å
            next_cell_col: ä¸‹ä¸€ä¸ªç»†èƒ ID åˆ—å
            index_col: CSV/Excel æ–‡ä»¶çš„ç´¢å¼•åˆ—
            preprocess: æ˜¯å¦è¿›è¡Œé¢„å¤„ç†ï¼ˆå½’ä¸€åŒ– + log1pï¼‰
            target_sum: å½’ä¸€åŒ–ç›®æ ‡æ€»è®¡æ•°ï¼ˆé»˜è®¤ 10,000ï¼Œå‚è€ƒ scimilarityï¼‰
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
        """
        if isinstance(data, (str, Path)):
            self.adata = load_anndata(
                data,
                index_col=index_col,
                preprocess=preprocess,
                target_sum=target_sum,
                verbose=verbose
            )
        else:
            self.adata = data
            # å¦‚æœä¼ å…¥çš„æ˜¯ AnnData å¯¹è±¡ï¼Œä¹Ÿè¿›è¡Œé¢„å¤„ç†
            if preprocess:
                self.adata = preprocess_counts(
                    self.adata,
                    target_sum=target_sum,
                    verbose=verbose
                )
        
        self.n_cells, self.n_genes = self.adata.shape
        self.time_col = time_col
        self.next_cell_col = next_cell_col
        
        # æ£€æŸ¥å¿…è¦åˆ—
        missing = [c for c in (time_col, next_cell_col) if c not in self.adata.obs.columns]
        if missing:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„ obs åˆ—: {missing}")
        
        # ç»†èƒåå’Œç´¢å¼•æ˜ å°„
        self.cell_names = self.adata.obs.index.to_list()
        self.name_to_idx = {name: i for i, name in enumerate(self.cell_names)}
        
        # æœ‰æ•ˆæ—¶åºå¯¹ç´¢å¼•
        if valid_pairs_only:
            valid_mask = ~pd.isna(self.adata.obs[next_cell_col])
            self.valid_indices = np.where(valid_mask)[0]
        else:
            self.valid_indices = np.arange(self.n_cells)
        
        if verbose:
            print(f"TemporalCellDataset åˆå§‹åŒ–å®Œæˆ:")
            print(f"  - ç»†èƒæ•°: {self.n_cells}")
            print(f"  - åŸºå› æ•°: {self.n_genes}")
            print(f"  - æœ‰æ•ˆæ—¶åºå¯¹: {len(self.valid_indices)} / {self.n_cells}")
        
        # è¡¨è¾¾çŸ©é˜µ -> tensor
        X = self.adata.X.toarray() if hasattr(self.adata.X, "toarray") else self.adata.X
        self.expressions = torch.tensor(X, dtype=torch.float32)
        
        # æ—¶é—´ä¿¡æ¯
        self.times = torch.tensor(self.adata.obs[time_col].to_numpy(), dtype=torch.float32)
        # TODO: æ ¹æ®å®é™…æ•°æ®è°ƒæ•´æ—¶é—´å½’ä¸€åŒ–
        if self.times.max() > 10:  # å¦‚æœæ—¶é—´å€¼å¾ˆå¤§ï¼Œè¿›è¡Œå½’ä¸€åŒ–
            self.times = self.times / 100.0
        
        # ä¸‹ä¸€ä¸ªç»†èƒ ID
        self.next_cell_ids = self.adata.obs[next_cell_col].to_numpy()
        # TODO: ä¸´æ—¶å¤„ç†ï¼Œå°† -1 æ›¿æ¢ä¸º 0
        self.next_cell_ids[self.next_cell_ids == -1] = 0
    
    def __len__(self) -> int:
        return len(self.valid_indices)
    
    def _resolve_next_index(self, raw_id, fallback_idx: int) -> int:
        """
        è§£æä¸‹ä¸€ä¸ªç»†èƒçš„ç´¢å¼•
        
        Args:
            raw_id: åŸå§‹ IDï¼ˆå¯èƒ½æ˜¯åç§°æˆ–ç´¢å¼•ï¼‰
            fallback_idx: å¤‡ç”¨ç´¢å¼•
        
        Returns:
            è§£æåçš„ç´¢å¼•
        """
        # TODO: æ ¹æ®å®é™…æ•°æ®æ ¼å¼å®Œå–„
        idx = int(raw_id) if not pd.isna(raw_id) else fallback_idx
        return idx
    
    def __getitem__(self, idx: int):
        """
        è¿”å›æ—¶åºå¯¹
        
        Returns:
            dict: åŒ…å«ä»¥ä¸‹é”®
                - x_cur: [n_genes] å½“å‰ç»†èƒè¡¨è¾¾
                - x_next: [n_genes] ä¸‹ä¸€ä¸ªç»†èƒè¡¨è¾¾
                - t_cur: å½“å‰æ—¶é—´
                - t_next: ä¸‹ä¸€ä¸ªæ—¶é—´
        """
        # å½“å‰ç»†èƒç´¢å¼•
        current_idx = int(self.valid_indices[idx])
        
        x_cur = self.expressions[current_idx]
        t_cur = self.times[current_idx]
        
        # è§£æä¸‹ä¸€ä¸ªç»†èƒç´¢å¼•
        raw_next_id = self.next_cell_ids[current_idx]
        next_idx = self._resolve_next_index(raw_next_id, fallback_idx=current_idx)
        
        x_next = self.expressions[next_idx]
        t_next = self.times[next_idx]
        
        return {
            "x_cur": x_cur,
            "x_next": x_next,
            "t_cur": t_cur,
            "t_next": t_next,
        }


class MultiCellDataset(Dataset):
    """
    å¤šç»†èƒæ•°æ®é›†
    æ¯æ¬¡é‡‡æ ·å¤šä¸ªç»†èƒçš„è¡¨è¾¾å‘é‡ï¼ˆç”¨äºé›†åˆç¼–ç å™¨ï¼‰
    """
    
    def __init__(
        self,
        data: Union[str, Path, "ad.AnnData"],
        *,
        n_cells_per_sample: int = 5,
        sampling_with_replacement: bool = True,
        index_col: int = 0,
        preprocess: bool = True,
        target_sum: float = 1e4,
        verbose: bool = False,
        seed: Optional[int] = None,
    ):
        """
        Args:
            data: æ•°æ®è·¯å¾„æˆ– AnnData å¯¹è±¡
            n_cells_per_sample: æ¯æ¬¡é‡‡æ ·çš„ç»†èƒæ•°é‡
            sampling_with_replacement: æ˜¯å¦æœ‰æ”¾å›é‡‡æ ·
            index_col: int = 0,
            preprocess: æ˜¯å¦è¿›è¡Œé¢„å¤„ç†ï¼ˆå½’ä¸€åŒ– + log1pï¼‰
            target_sum: å½’ä¸€åŒ–ç›®æ ‡æ€»è®¡æ•°ï¼ˆé»˜è®¤ 10,000ï¼Œå‚è€ƒ scimilarityï¼‰
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
            seed: éšæœºç§å­
        """
        if isinstance(data, (str, Path)):
            self.adata = load_anndata(
                data,
                index_col=index_col,
                preprocess=preprocess,
                target_sum=target_sum,
                verbose=verbose
            )
        else:
            self.adata = data
            # å¦‚æœä¼ å…¥çš„æ˜¯ AnnData å¯¹è±¡ï¼Œä¹Ÿè¿›è¡Œé¢„å¤„ç†
            if preprocess:
                self.adata = preprocess_counts(
                    self.adata,
                    target_sum=target_sum,
                    verbose=verbose
                )
        
        self.n_cells, self.n_genes = self.adata.shape
        self.n_cells_per_sample = n_cells_per_sample
        self.sampling_with_replacement = sampling_with_replacement
        
        if not sampling_with_replacement and n_cells_per_sample > self.n_cells:
            raise ValueError(
                f"æ— æ”¾å›é‡‡æ ·æ—¶ï¼Œæ¯æ¬¡é‡‡æ ·ç»†èƒæ•° ({n_cells_per_sample}) "
                f"ä¸èƒ½è¶…è¿‡æ€»ç»†èƒæ•° ({self.n_cells})"
            )
        
        # è¡¨è¾¾çŸ©é˜µ -> tensor
        X = self.adata.X.toarray() if hasattr(self.adata.X, "toarray") else self.adata.X
        self.expressions = torch.tensor(X, dtype=torch.float32)
        
        # éšæœºæ•°ç”Ÿæˆå™¨
        self.rng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()
        
        if verbose:
            print(f"MultiCellDataset åˆå§‹åŒ–å®Œæˆ:")
            print(f"  - ç»†èƒæ•°: {self.n_cells}")
            print(f"  - åŸºå› æ•°: {self.n_genes}")
            print(f"  - æ¯æ¬¡é‡‡æ ·ç»†èƒæ•°: {n_cells_per_sample}")
            print(f"  - é‡‡æ ·æ–¹å¼: {'æœ‰æ”¾å›' if sampling_with_replacement else 'æ— æ”¾å›'}")
    
    def __len__(self) -> int:
        return self.n_cells
    
    def __getitem__(self, idx: int):
        """
        è¿”å›å¤šä¸ªç»†èƒçš„è¡¨è¾¾å‘é‡
        
        Returns:
            dict: åŒ…å«ä»¥ä¸‹é”®
                - expressions: [n_cells_per_sample, n_genes] è¡¨è¾¾çŸ©é˜µ
                - cell_indices: [n_cells_per_sample] ç»†èƒç´¢å¼•
        """
        if self.sampling_with_replacement:
            sampled_indices = self.rng.integers(
                0, self.n_cells, size=self.n_cells_per_sample
            )
        else:
            sampled_indices = self.rng.choice(
                self.n_cells, size=self.n_cells_per_sample, replace=False
            )
        
        sampled_expressions = self.expressions[sampled_indices]
        
        return {
            "expressions": sampled_expressions,
            "cell_indices": torch.tensor(sampled_indices, dtype=torch.long),
        }


# ==================== Collate Functions ====================

def collate_fn_static(batch):
    """
    é™æ€æ•°æ®é›†çš„ collate å‡½æ•°
    
    Args:
        batch: list of tensors [n_genes]
    
    Returns:
        tensor: [batch_size, n_genes]
    """
    return torch.stack(batch, dim=0)


def collate_fn_temporal(batch):
    """
    æ—¶åºæ•°æ®é›†çš„ collate å‡½æ•°
    
    Args:
        batch: list of dicts
    
    Returns:
        dict: æ‰¹é‡æ•°æ®
    """
    return {
        "x_cur": torch.stack([b["x_cur"] for b in batch]),
        "x_next": torch.stack([b["x_next"] for b in batch]),
        "t_cur": torch.stack([b["t_cur"] for b in batch]),
        "t_next": torch.stack([b["t_next"] for b in batch]),
    }


def collate_fn_multi_cell(batch):
    """
    å¤šç»†èƒæ•°æ®é›†çš„ collate å‡½æ•°
    
    Args:
        batch: list of dicts
    
    Returns:
        dict: æ‰¹é‡æ•°æ®
    """
    return {
        "expressions": torch.stack([b["expressions"] for b in batch]),
        "cell_indices": torch.stack([b["cell_indices"] for b in batch]),
    }

