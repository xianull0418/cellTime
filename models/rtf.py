"""
Rectified Flow æ¨¡å‹
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šDirect å’Œ Inversion
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from typing import Optional, Dict, Any, List
from pathlib import Path
from omegaconf import DictConfig, OmegaConf

from models.utils import create_backbone


class RectifiedFlow(nn.Module):
    """
    Rectified Flow åŸºç±»
    å®ç°æ ¸å¿ƒçš„ Rectified Flow ç®—æ³•
    """
    
    def __init__(
        self,
        backbone: nn.Module,
        ln_noise: bool = True,
        normalize_latent: bool = True,
    ):
        """
        Args:
            backbone: é€Ÿåº¦åœºé¢„æµ‹å™¨ï¼ˆéª¨å¹²ç½‘ç»œï¼‰
            ln_noise: æ˜¯å¦ä½¿ç”¨ log-normal å™ªå£°åˆ†å¸ƒé‡‡æ ·æ—¶é—´
            normalize_latent: æ˜¯å¦å½’ä¸€åŒ–æ½œç©ºé—´å‘é‡ï¼ˆç”¨äº scimilarity encoderï¼‰
        """
        super().__init__()
        self.backbone = backbone
        self.ln_noise = ln_noise
        self.normalize_latent = normalize_latent
    
    def sample_timestep(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """
        é‡‡æ ·æ—¶é—´æ­¥ t âˆˆ [0, 1]
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
        
        Returns:
            æ—¶é—´æ­¥ [B]
        """
        if self.ln_noise:
            # Log-normal åˆ†å¸ƒï¼ˆæ›´å…³æ³¨ä¸­é—´æ—¶é—´æ­¥ï¼‰
            nt = torch.randn(batch_size, device=device)
            t = torch.sigmoid(nt)
        else:
            # å‡åŒ€åˆ†å¸ƒ
            t = torch.rand(batch_size, device=device)
        return t
    
    @torch.no_grad()
    def sample(
        self,
        z_start: torch.Tensor,
        sample_steps: int = 50,
        cond: Optional[torch.Tensor] = None,
        null_cond: Optional[torch.Tensor] = None,
        cfg_scale: float = 2.0,
    ) -> List[torch.Tensor]:
        """
        ä»èµ·ç‚¹é‡‡æ ·åˆ°ç»ˆç‚¹ï¼ˆéœ€è¦åœ¨å­ç±»ä¸­å®ç°å…·ä½“é€»è¾‘ï¼‰
        
        Args:
            z_start: èµ·å§‹æ½œç©ºé—´ [B, latent_dim]
            sample_steps: é‡‡æ ·æ­¥æ•°
            cond: æ¡ä»¶ä¿¡æ¯
            null_cond: æ— æ¡ä»¶ä¿¡æ¯ï¼ˆç”¨äº CFGï¼‰
            cfg_scale: CFG å¼ºåº¦
        
        Returns:
            é‡‡æ ·è½¨è¿¹åˆ—è¡¨
        """
        raise NotImplementedError


class RFDirect(RectifiedFlow):
    """
    Direct æ¨¡å¼ï¼šz1 -> z2
    ç›´æ¥ä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„çº¿æ€§æ’å€¼
    """
    
    def forward(
        self,
        z1: torch.Tensor,
        z2: torch.Tensor,
        cond: Optional[torch.Tensor] = None,
    ) -> tuple:
        """
        è®¡ç®— Direct æ¨¡å¼çš„æŸå¤±
        
        Args:
            z1: èµ·ç‚¹æ½œç©ºé—´ [B, latent_dim]
            z2: ç»ˆç‚¹æ½œç©ºé—´ [B, latent_dim]
            cond: å¯é€‰æ¡ä»¶ä¿¡æ¯
        
        Returns:
            loss: æŸå¤±å€¼
            loss_dict: æŸå¤±å­—å…¸ï¼ˆç”¨äºè®°å½•ï¼‰
        """
        batch_size = z1.shape[0]
        device = z1.device
        
        # é‡‡æ ·æ—¶é—´æ­¥
        t = self.sample_timestep(batch_size, device)
        t_exp = t.view(batch_size, *([1] * (z1.ndim - 1)))  # [B, 1, ...]
        
        # çº¿æ€§æ’å€¼ï¼šz_t = (1-t) * z1 + t * z2
        z_t = (1 - t_exp) * z1 + t_exp * z2
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœä½¿ç”¨ scimilarity encoderï¼Œè®­ç»ƒæ—¶ä¹Ÿéœ€è¦å½’ä¸€åŒ–
        # ä¿æŒè®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§
        if self.normalize_latent:
            # æ·»åŠ å°çš„å™ªå£°ä»¥é¿å…é›¶å‘é‡ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
            z_t = z_t + 1e-8 * torch.randn_like(z_t)
            z_t = F.normalize(z_t, p=2, dim=1)
        
        # é¢„æµ‹é€Ÿåº¦åœº
        v_pred = self.backbone(z_t, t, cond)
        
        # ç›®æ ‡é€Ÿåº¦åœºï¼šv = z2 - z1
        v_target = z2 - z1
        
        # è®¡ç®—æŸå¤±ï¼ˆæ¯ä¸ªæ ·æœ¬ï¼‰
        batchwise_loss = F.mse_loss(v_pred, v_target, reduction='none').mean(dim=list(range(1, v_pred.ndim)))
        
        # æ€»æŸå¤±
        loss = batchwise_loss.mean()
        
        # ç”¨äºè®°å½•çš„æŸå¤±å­—å…¸
        loss_dict = [(t[i].item(), batchwise_loss[i].item()) for i in range(batch_size)]
        
        return loss, loss_dict
    
    @torch.no_grad()
    def sample(
        self,
        z_start: torch.Tensor,
        sample_steps: int = 50,
        cond: Optional[torch.Tensor] = None,
        null_cond: Optional[torch.Tensor] = None,
        cfg_scale: float = 2.0,
        normalize_latent: bool = True,
    ) -> List[torch.Tensor]:
        """
        ä» z1 é‡‡æ ·åˆ° z2
        
        Args:
            z_start: èµ·ç‚¹ z1 [B, latent_dim]
            sample_steps: é‡‡æ ·æ­¥æ•°
            cond: æ¡ä»¶ä¿¡æ¯
            null_cond: æ— æ¡ä»¶ä¿¡æ¯ï¼ˆç”¨äº CFGï¼‰
            cfg_scale: CFG å¼ºåº¦
            normalize_latent: æ˜¯å¦åœ¨æ¯æ­¥åå½’ä¸€åŒ–æ½œç©ºé—´å‘é‡ï¼ˆç”¨äº scimilarityï¼‰
        
        Returns:
            é‡‡æ ·è½¨è¿¹ [z_0, z_1, ..., z_T]
        """
        z = z_start.clone()
        batch_size = z.shape[0]
        device = z.device
        dt = 1.0 / sample_steps
        
        trajectory = [z.cpu()]
        
        for step in range(sample_steps):
            t_current = step / sample_steps
            t = torch.full((batch_size,), t_current, device=device)
            
            # é¢„æµ‹é€Ÿåº¦åœº
            v = self.backbone(z, t, cond)
            
            # Classifier-Free Guidance
            if null_cond is not None and cfg_scale != 1.0:
                v_uncond = self.backbone(z, t, null_cond)
                v = v_uncond + cfg_scale * (v - v_uncond)
            
            # æ¬§æ‹‰æ­¥ï¼šz = z + v * dt
            z = z + v * dt
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœä½¿ç”¨ scimilarity encoderï¼Œéœ€è¦å½’ä¸€åŒ–åˆ°å•ä½çƒé¢
            if normalize_latent:
                # æ·»åŠ å°çš„å™ªå£°ä»¥é¿å…é›¶å‘é‡ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
                z = z + 1e-8 * torch.randn_like(z)
                z = F.normalize(z, p=2, dim=1)
            
            trajectory.append(z.cpu())
        
        return trajectory


class RFInversion(RectifiedFlow):
    """
    Inversion æ¨¡å¼ï¼šz1 -> noise -> z2
    å…ˆåæ¼”åˆ°å™ªå£°ç©ºé—´ï¼Œå†ä»å™ªå£°ç”Ÿæˆç›®æ ‡
    """
    
    def forward(
        self,
        z1: torch.Tensor,
        z2: torch.Tensor,
        cond1: Optional[torch.Tensor] = None,
        cond2: Optional[torch.Tensor] = None,
    ) -> tuple:
        """
        è®¡ç®— Inversion æ¨¡å¼çš„æŸå¤±
        è®­ç»ƒä¸¤ä¸ªæ–¹å‘ï¼šz1->noise å’Œ z2->noise
        
        Args:
            z1: èµ·ç‚¹æ½œç©ºé—´ [B, latent_dim]
            z2: ç»ˆç‚¹æ½œç©ºé—´ [B, latent_dim]
            cond1: z1 çš„æ¡ä»¶ä¿¡æ¯
            cond2: z2 çš„æ¡ä»¶ä¿¡æ¯
        
        Returns:
            loss: æŸå¤±å€¼
            loss_dict: æŸå¤±å­—å…¸
        """
        batch_size = z1.shape[0]
        device = z1.device
        
        # é‡‡æ ·æ—¶é—´æ­¥
        t = self.sample_timestep(batch_size, device)
        t_exp = t.view(batch_size, *([1] * (z1.ndim - 1)))
        
        # åˆ†åˆ«è®­ç»ƒä¸¤ä¸ªæ–¹å‘
        # æ–¹å‘ 1ï¼šz1 -> noise
        noise1 = torch.randn_like(z1)
        if self.normalize_latent:
            noise1 = F.normalize(noise1, p=2, dim=1)  # å™ªå£°ä¹Ÿå½’ä¸€åŒ–åˆ°å•ä½çƒé¢
        z_t1 = (1 - t_exp) * z1 + t_exp * noise1
        if self.normalize_latent:
            z_t1 = z_t1 + 1e-8 * torch.randn_like(z_t1)  # æ•°å€¼ç¨³å®šæ€§
            z_t1 = F.normalize(z_t1, p=2, dim=1)
        v_pred1 = self.backbone(z_t1, t, cond1)
        v_target1 = noise1 - z1
        
        # æ–¹å‘ 2ï¼šz2 -> noise
        noise2 = torch.randn_like(z2)
        if self.normalize_latent:
            noise2 = F.normalize(noise2, p=2, dim=1)  # å™ªå£°ä¹Ÿå½’ä¸€åŒ–åˆ°å•ä½çƒé¢
        z_t2 = (1 - t_exp) * z2 + t_exp * noise2
        if self.normalize_latent:
            z_t2 = z_t2 + 1e-8 * torch.randn_like(z_t2)  # æ•°å€¼ç¨³å®šæ€§
            z_t2 = F.normalize(z_t2, p=2, dim=1)
        v_pred2 = self.backbone(z_t2, t, cond2)
        v_target2 = noise2 - z2
        
        # åˆå¹¶è®¡ç®—æŸå¤±
        z_t = torch.cat([z_t1, z_t2], dim=0)
        v_pred = torch.cat([v_pred1, v_pred2], dim=0)
        v_target = torch.cat([v_target1, v_target2], dim=0)
        
        # è®¡ç®—æŸå¤±
        batchwise_loss = F.mse_loss(v_pred, v_target, reduction='none').mean(dim=list(range(1, v_pred.ndim)))
        loss = batchwise_loss.mean()
        
        # æŸå¤±å­—å…¸
        t_full = torch.cat([t, t], dim=0)
        loss_dict = [(t_full[i].item(), batchwise_loss[i].item()) for i in range(len(t_full))]
        
        return loss, loss_dict
    
    @torch.no_grad()
    def sample(
        self,
        z_start: torch.Tensor,
        sample_steps: int = 50,
        cond_start: Optional[torch.Tensor] = None,
        cond_target: Optional[torch.Tensor] = None,
        null_cond: Optional[torch.Tensor] = None,
        cfg_scale: float = 2.0,
        normalize_latent: bool = True,
    ) -> List[torch.Tensor]:
        """
        ä» z1 åæ¼”åˆ°å™ªå£°ï¼Œå†ä»å™ªå£°ç”Ÿæˆ z2
        
        Args:
            z_start: èµ·ç‚¹ z1 [B, latent_dim]
            sample_steps: é‡‡æ ·æ­¥æ•°ï¼ˆæ¯ä¸ªé˜¶æ®µï¼‰
            cond_start: z1 çš„æ¡ä»¶ä¿¡æ¯
            cond_target: z2 çš„æ¡ä»¶ä¿¡æ¯
            null_cond: æ— æ¡ä»¶ä¿¡æ¯ï¼ˆç”¨äº CFGï¼‰
            cfg_scale: CFG å¼ºåº¦
            normalize_latent: æ˜¯å¦åœ¨æ¯æ­¥åå½’ä¸€åŒ–æ½œç©ºé—´å‘é‡ï¼ˆç”¨äº scimilarityï¼‰
        
        Returns:
            é‡‡æ ·è½¨è¿¹
        """
        z = z_start.clone()
        batch_size = z.shape[0]
        device = z.device
        dt = 1.0 / sample_steps
        
        trajectory = []
        
        # é˜¶æ®µ 1ï¼šz1 -> noiseï¼ˆæ­£å‘ï¼‰
        for step in range(sample_steps):
            t_current = step / sample_steps
            t = torch.full((batch_size,), t_current, device=device)
            
            v = self.backbone(z, t, cond_start)
            
            if null_cond is not None and cfg_scale != 1.0:
                v_uncond = self.backbone(z, t, null_cond)
                v = v_uncond + cfg_scale * (v - v_uncond)
            
            z = z + v * dt
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœä½¿ç”¨ scimilarity encoderï¼Œéœ€è¦å½’ä¸€åŒ–åˆ°å•ä½çƒé¢
            if normalize_latent:
                z = z + 1e-8 * torch.randn_like(z)  # æ•°å€¼ç¨³å®šæ€§
                z = F.normalize(z, p=2, dim=1)
            
            trajectory.append(z.cpu())
        
        # é˜¶æ®µ 2ï¼šnoise -> z2ï¼ˆåå‘ï¼‰
        for step in range(sample_steps):
            t_current = 1.0 - step / sample_steps
            t = torch.full((batch_size,), t_current, device=device)
            
            v = self.backbone(z, t, cond_target)
            
            if null_cond is not None and cfg_scale != 1.0:
                v_uncond = self.backbone(z, t, null_cond)
                v = v_uncond + cfg_scale * (v - v_uncond)
            
            z = z - v * dt
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœä½¿ç”¨ scimilarity encoderï¼Œéœ€è¦å½’ä¸€åŒ–åˆ°å•ä½çƒé¢
            if normalize_latent:
                z = z + 1e-8 * torch.randn_like(z)  # æ•°å€¼ç¨³å®šæ€§
                z = F.normalize(z, p=2, dim=1)
            
            trajectory.append(z.cpu())
        
        return trajectory


class RTFSystem(pl.LightningModule):
    """
    Rectified Flow è®­ç»ƒç³»ç»Ÿï¼ˆPyTorch Lightningï¼‰
    """
    
    def __init__(
        self,
        cfg: DictConfig,
        ae_encoder: nn.Module,
        ae_decoder: nn.Module,
    ):
        """
        Args:
            cfg: é…ç½®å¯¹è±¡ï¼ˆOmegaConf æˆ–å­—å…¸ï¼‰
            ae_encoder: é¢„è®­ç»ƒçš„ AE Encoderï¼ˆå·²å†»ç»“ï¼‰
            ae_decoder: é¢„è®­ç»ƒçš„ AE Decoderï¼ˆå·²å†»ç»“ï¼Œç”¨äºè®¡ç®—é‡å»ºè¯¯å·®ï¼‰
        """
        super().__init__()
        
        # å¦‚æœ cfg æ˜¯ OmegaConf å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸ï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨
        if isinstance(cfg, DictConfig):
            self.save_hyperparameters(OmegaConf.to_container(cfg, resolve=True), ignore=['ae_encoder', 'ae_decoder'])
            self.cfg = cfg
        else:
            # ä» checkpoint åŠ è½½æ—¶ï¼Œcfg å·²ç»æ˜¯å­—å…¸
            self.save_hyperparameters(cfg, ignore=['ae_encoder', 'ae_decoder'])
            self.cfg = OmegaConf.create(cfg)
        
        # ç»Ÿä¸€ä½¿ç”¨ self.cfg è®¿é—®é…ç½®
        cfg = self.cfg
        
        # ä¿å­˜ AE Encoder å’Œ Decoder
        self.ae_encoder = ae_encoder
        self.ae_encoder.eval()
        for param in self.ae_encoder.parameters():
            param.requires_grad = False
        
        self.ae_decoder = ae_decoder
        self.ae_decoder.eval()
        for param in self.ae_decoder.parameters():
            param.requires_grad = False
        
        # åŠ è½½éª¨å¹²ç½‘ç»œé…ç½®
        backbone_config_path = f"config/backbones/{cfg.model.backbone}.yaml"
        backbone_cfg = OmegaConf.load(backbone_config_path)
        backbone_cfg_dict = OmegaConf.to_container(backbone_cfg, resolve=True)
        
        # æ³¨å…¥æ¡ä»¶é…ç½®
        if cfg.model.use_cond and cfg.model.cond_dim is not None:
            if backbone_cfg_dict.get('use_class_cond', False):
                print("Warning: use_class_cond is enabled in backbone config but use_cond is also enabled.")
                print("Disabling use_class_cond and enabling use_vector_cond.")
                backbone_cfg_dict['use_class_cond'] = False
            
            backbone_cfg_dict['use_vector_cond'] = True
            backbone_cfg_dict['vector_cond_dim'] = cfg.model.cond_dim
            print(f"å¯ç”¨å‘é‡æ¡ä»¶: dim={cfg.model.cond_dim}")
        
        # åˆ›å»ºéª¨å¹²ç½‘ç»œ
        backbone = create_backbone(
            cfg.model.backbone,
            backbone_cfg_dict,
            cfg.model.latent_dim
        )
        
        # åˆ›å»º RTF æ¨¡å‹
        normalize_latent = getattr(cfg.model, 'normalize_latent', True)
        
        if cfg.model.mode == "direct":
            self.model = RFDirect(
                backbone, 
                ln_noise=cfg.model.ln_noise,
                normalize_latent=normalize_latent
            )
        elif cfg.model.mode == "inversion":
            self.model = RFInversion(
                backbone, 
                ln_noise=cfg.model.ln_noise,
                normalize_latent=normalize_latent
            )
        else:
            raise ValueError(f"Unknown mode: {cfg.model.mode}")
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(cfg.logging.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”¨äºç»Ÿè®¡æŸå¤±åˆ†å¸ƒ
        self._reset_loss_bins()
    
    def _reset_loss_bins(self):
        """é‡ç½®æŸå¤±ç»Ÿè®¡æ¡¶"""
        self.loss_bins = {i: 0.0 for i in range(10)}
        self.loss_counts = {i: 1e-6 for i in range(10)}
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.cfg.training.learning_rate,
            weight_decay=self.cfg.training.weight_decay,
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.cfg.training.scheduler.type == "cosine":
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.cfg.training.scheduler.T_max,
            )
        else:
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=0.5,
                patience=10,
            )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch",
            },
        }
    # test
    # æµ‹è¯•
    def setup(self, stage: Optional[str] = None):
        """è®¾ç½®æ•°æ®é›†"""
        if stage == "fit" or stage is None:
            from dataset import TemporalCellDataset
            
            # å°è¯•åŠ è½½ç›®æ ‡åŸºå› åˆ—è¡¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            target_genes = None
            if hasattr(self.cfg.data, 'target_genes_path') and self.cfg.data.target_genes_path:
                path = Path(self.cfg.data.target_genes_path)
                if path.exists():
                    print(f"åŠ è½½ç›®æ ‡åŸºå› åˆ—è¡¨: {path}")
                    with open(path, 'r') as f:
                        target_genes = [line.strip() for line in f if line.strip()]
                    print(f"ç›®æ ‡åŸºå› æ•°é‡: {len(target_genes)}")
            
            # ä½¿ç”¨ AE çš„åŸºå› æ•°ä½œä¸º max_genesï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            max_genes = self.cfg.model.n_genes if hasattr(self.cfg.model, 'n_genes') and self.cfg.model.n_genes > 0 else None
            
            self.train_dataset = TemporalCellDataset(
                data=self.cfg.data.data_path,
                max_genes=max_genes,
                target_genes=target_genes,
                valid_pairs_only=self.cfg.data.valid_pairs_only,
                time_col=self.cfg.data.time_col,
                next_cell_col=self.cfg.data.next_cell_col,
                verbose=True,
            )
            
            print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(self.train_dataset)}")
            print(f"åŸºå› æ•°é‡: {self.train_dataset.n_genes}")
            
            # éªŒè¯åŸºå› æ•°ä¸ AE ä¸€è‡´
            if max_genes is not None and self.train_dataset.n_genes != max_genes:
                print(f"Warning: æ•°æ®é›†åŸºå› æ•° ({self.train_dataset.n_genes}) ä¸é¢„æœŸ ({max_genes}) ä¸ä¸€è‡´")
    
    def train_dataloader(self):
        """è®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        from dataset import collate_fn_temporal
        
        return DataLoader(
            self.train_dataset,
            batch_size=self.cfg.training.batch_size,
            shuffle=True,
            drop_last=True,
            num_workers=self.cfg.data.num_workers,
            pin_memory=self.cfg.data.pin_memory,
            collate_fn=collate_fn_temporal,
        )
    
    @torch.no_grad()
    def encode_to_latent(self, x: torch.Tensor) -> torch.Tensor:
        """ä½¿ç”¨ AE Encoder ç¼–ç åˆ°æ½œç©ºé—´"""
        return self.ae_encoder(x)
    
    @torch.no_grad()
    def decode_from_latent(self, z: torch.Tensor) -> torch.Tensor:
        """ä½¿ç”¨ AE Decoder ä»æ½œç©ºé—´è§£ç """
        return self.ae_decoder(z)
    
    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):
        """è®­ç»ƒæ­¥éª¤"""
        x_cur = batch["x_cur"]  # [B, n_genes]
        x_next = batch["x_next"]  # [B, n_genes]
        t_cur = batch["t_cur"]  # [B]
        t_next = batch["t_next"]  # [B]
        
        # ç¼–ç åˆ°æ½œç©ºé—´
        with torch.no_grad():
            z_cur = self.encode_to_latent(x_cur)
            z_next = self.encode_to_latent(x_next)
        
        # ğŸ” è¯Šæ–­ä¿¡æ¯ï¼ˆæ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼‰
        if batch_idx % 100 == 0:
            with torch.no_grad():
                print(f"\n[è¯Šæ–­ Batch {batch_idx}]")
                print(f"  åŸå§‹ç©ºé—´ x_cur: min={x_cur.min():.4f}, max={x_cur.max():.4f}, "
                      f"mean={x_cur.mean():.4f}, std={x_cur.std():.4f}")
                print(f"  åŸå§‹ç©ºé—´ x_next: min={x_next.min():.4f}, max={x_next.max():.4f}, "
                      f"mean={x_next.mean():.4f}, std={x_next.std():.4f}")
                print(f"  æ½œç©ºé—´ z_cur: min={z_cur.min():.4f}, max={z_cur.max():.4f}, "
                      f"mean={z_cur.mean():.4f}, std={z_cur.std():.4f}")
                print(f"  æ½œç©ºé—´ z_next: min={z_next.min():.4f}, max={z_next.max():.4f}, "
                      f"mean={z_next.mean():.4f}, std={z_next.std():.4f}")
                print(f"  é€Ÿåº¦åœº v_target (z_next - z_cur): "
                      f"norm_mean={torch.norm(z_next - z_cur, dim=-1).mean():.6f}")
        
        # å‡†å¤‡æ¡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        cond = None
        if self.cfg.model.use_cond:
            cond = torch.stack([t_cur, t_next], dim=-1)  # [B, 2]
        
        # è®¡ç®—æŸå¤±
        if self.cfg.model.mode == "direct":
            loss, loss_dict = self.model(z_cur, z_next, cond)
        else:  # inversion
            cond1 = torch.stack([t_cur], dim=-1) if self.cfg.model.use_cond else None
            cond2 = torch.stack([t_next], dim=-1) if self.cfg.model.use_cond else None
            loss, loss_dict = self.model(z_cur, z_next, cond1, cond2)
        
        # è®°å½•æŸå¤±
        self.log("train_loss", loss, prog_bar=True, on_step=True, on_epoch=True)
        self.log("lr", self.optimizers().param_groups[0]["lr"], on_step=True)
        
        # ç»Ÿè®¡æŸå¤±åˆ†å¸ƒ
        for t_val, l_val in loss_dict:
            bin_idx = int(t_val * 10)
            if 0 <= bin_idx < 10:
                self.loss_bins[bin_idx] += l_val
                self.loss_counts[bin_idx] += 1.0
        
        return loss
    
    def on_train_epoch_end(self):
        """è®­ç»ƒ epoch ç»“æŸ"""
        # è®°å½•å„æ—¶é—´æ®µæŸå¤±
        for i in range(10):
            avg_loss = self.loss_bins[i] / self.loss_counts[i]
            self.log(f"loss_bin_{i}", avg_loss, prog_bar=False, on_epoch=True)
        
        # é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.current_epoch % self.cfg.training.sample_every_n_epochs == 0:
            self._sample_and_save(self.current_epoch)
        
        # é‡ç½®ç»Ÿè®¡
        self._reset_loss_bins()
    
    @torch.no_grad()
    def _sample_and_save(self, epoch: int):
        """é‡‡æ ·å¹¶ä¿å­˜"""
        self.model.eval()
        
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(self.train_dataloader()))
        x_cur = batch["x_cur"][:8].to(self.device)
        x_next = batch["x_next"][:8].to(self.device)
        t_cur = batch["t_cur"][:8].to(self.device)
        t_next = batch["t_next"][:8].to(self.device)
        
        # ç¼–ç 
        z_cur = self.encode_to_latent(x_cur)
        z_next = self.encode_to_latent(x_next)
        
        # å‡†å¤‡æ¡ä»¶
        cond = None
        null_cond = None
        
        if self.cfg.model.use_cond:
            if self.cfg.model.mode == "direct":
                cond = torch.stack([t_cur, t_next], dim=-1)
                null_cond = torch.zeros_like(cond)
            else:
                # Inversion æ¨¡å¼ä¸‹ï¼Œæ¡ä»¶ç»´åº¦æ˜¯ 1 (å•ç‹¬çš„æ—¶é—´ç‚¹)
                # cond_start å’Œ cond_target åˆ†åˆ«æ„å»º
                # null_cond åº”è¯¥ä¸ cond_start/target ç»´åº¦ä¸€è‡´ [B, 1]
                null_cond = torch.zeros(x_cur.shape[0], 1, device=self.device)
        
        # é‡‡æ ·ï¼ˆå¦‚æœä½¿ç”¨ scimilarity encoderï¼Œéœ€è¦å½’ä¸€åŒ–ï¼‰
        normalize_latent = getattr(self.cfg.model, 'normalize_latent', True)
        
        if self.cfg.model.mode == "direct":
            trajectory = self.model.sample(
                z_cur,
                sample_steps=self.cfg.training.sample_steps,
                cond=cond,
                null_cond=null_cond,
                cfg_scale=self.cfg.training.cfg_scale,
                normalize_latent=normalize_latent,
            )
        else:
            cond_start = torch.stack([t_cur], dim=-1) if self.cfg.model.use_cond else None
            cond_target = torch.stack([t_next], dim=-1) if self.cfg.model.use_cond else None
            trajectory = self.model.sample(
                z_cur,
                sample_steps=self.cfg.training.sample_steps,
                cond_start=cond_start,
                cond_target=cond_target,
                null_cond=null_cond,
                cfg_scale=self.cfg.training.cfg_scale,
                normalize_latent=normalize_latent,
            )
        
        # ğŸ”§ æ­£ç¡®è®¡ç®—é‡å»ºè¯¯å·®ï¼šåœ¨åŸå§‹ç©ºé—´è€Œä¸æ˜¯æ½œç©ºé—´
        z_final = trajectory[-1].to(self.device)
        
        # è§£ç åˆ°åŸå§‹ç©ºé—´
        x_reconstructed = self.decode_from_latent(z_final)
        
        # --- ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥ AE çš„ç†è®ºä¸Šé™ ---
        # ç›´æ¥é‡å»º z_next (ç›®æ ‡æ½œå‘é‡)ï¼Œçœ‹çœ‹ AE è‡ªå·±èƒ½ä¸èƒ½é‡å»ºå›å»
        x_next_ae_recon = self.decode_from_latent(z_next)
        ae_recon_error = F.mse_loss(x_next_ae_recon, x_next).item()
        from models.utils import compute_correlation
        ae_correlation = compute_correlation(x_next, x_next_ae_recon)
        
        print(f"  [DEBUG] AE ç›´æ¥é‡å»ºè¯¯å·®: {ae_recon_error:.6f}")
        print(f"  [DEBUG] AE ç›´æ¥é‡å»ºç›¸å…³æ€§: {ae_correlation:.4f}")
        self.log("ae_oracle_correlation", ae_correlation, on_epoch=True)
        # ----------------------------------
        
        # åœ¨åŸå§‹ç©ºé—´è®¡ç®—é‡å»ºè¯¯å·®
        recon_error_original = F.mse_loss(x_reconstructed, x_next).item()
        
        # åŒæ—¶è®°å½•æ½œç©ºé—´è¯¯å·®ç”¨äºå¯¹æ¯”
        recon_error_latent = F.mse_loss(z_final, z_next).item()
        
        # è®¡ç®—ç›¸å…³æ€§ï¼ˆè¡¡é‡é‡å»ºè´¨é‡ï¼‰
        from models.utils import compute_correlation
        correlation = compute_correlation(x_next, x_reconstructed)
        
        self.log("sample_recon_error_original", recon_error_original, on_epoch=True)
        self.log("sample_recon_error_latent", recon_error_latent, on_epoch=True)
        self.log("sample_correlation", correlation, on_epoch=True)
        
        print(f"Epoch {epoch}: é‡‡æ ·å®Œæˆ")
        print(f"  åŸå§‹ç©ºé—´é‡å»ºè¯¯å·®: {recon_error_original:.6f}")
        print(f"  æ½œç©ºé—´é‡å»ºè¯¯å·®: {recon_error_latent:.6f}")
        print(f"  é‡å»ºç›¸å…³æ€§: {correlation:.4f}")
        
        self.model.train()

