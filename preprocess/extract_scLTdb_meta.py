import pandas as pd
import os
import sys
import re
from typing import Dict, List, Any

def extract_dataset_info(csv_file: str, log_file: str, storage_path: str, data_source: str) -> pd.DataFrame:
    """
    ä»CSVå’Œæ—¥å¿—æ–‡ä»¶ä¸­æå–æ•°æ®é›†ä¿¡æ¯
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        storage_path: å­˜å‚¨è·¯å¾„
        data_source: æ•°æ®æ¥æº
        
    Returns:
        åŒ…å«æå–ä¿¡æ¯çš„DataFrame
    """
    
    # è¯»å–CSVæ–‡ä»¶
    csv_data = pd.read_csv(csv_file)
    
    # è§£ææ—¥å¿—æ–‡ä»¶æå–æ•°æ®åå’Œç»†èƒæ•°ç›®
    log_info = parse_log_file(log_file)
    
    # å‡†å¤‡ç»“æœåˆ—è¡¨
    results = []
    
    # å¤„ç†æ¯ä¸ªåœ¨æ—¥å¿—ä¸­å‡ºç°çš„æ•°æ®åº“
    for dataset_file_name, cell_count in log_info.items():
        # ä»æ–‡ä»¶åä¸­æå–åŒ¹é…CSV Datasetåˆ—çš„éƒ¨åˆ†
        dataset_match_name = dataset_file_name.replace('.link.h5ad', '')
        
        # åœ¨CSVçš„Datasetåˆ—ä¸­æŸ¥æ‰¾åŒ¹é…é¡¹
        matched_datasets = csv_data[csv_data['Dataset'] == dataset_match_name]
        
        if not matched_datasets.empty:
            # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ•°æ®é›†
            dataset_row = matched_datasets.iloc[0]
            
            # æå–æ—¶åºä¿¡æ¯
            time_info = extract_time_info(dataset_file_name, log_file)
            
            # æ„å»ºç»“æœå­—å…¸
            result = {
                'æ•°æ®å': dataset_file_name,
                'å­˜å‚¨è·¯å¾„': storage_path,
                'æ•°æ®ç±»å‹': 'h5ad',
                'æ•°æ®æ¥æº': data_source,
                'æ•°æ®ç”¨é€”': 'ç»†èƒè°±ç³»è¿½è¸ª',
                'å­˜å‚¨æ ¼å¼': 'h5ad',
                'ç»†èƒæ•°ç›®': cell_count,
                'ç‰©ç§': dataset_row['Species'],
                'ç»„ç»‡': dataset_row['Tissue source'],
                'æµ‹åºæŠ€æœ¯': dataset_row['Technology'],
                'å¥åº·/ç–¾ç—…': determine_health_status(dataset_row['Tissue source']),
                'æ˜¯å¦æ‰°åŠ¨': determine_perturbation(dataset_row['Dataset']),
                'æ‰°åŠ¨ç±»å‹': determine_perturbation_type(dataset_row['Dataset']),
                'æ‰°åŠ¨æ•°': 'å¾…è¡¥å……',
                'æ˜¯å¦å«æ—¶åºä¿¡æ¯': time_info['has_time'],
                'é‡‡æ ·ç‚¹æ•°ç›®': time_info['time_points'],
                'æ—¶åºå€¼': time_info['time_values'],
                'æ—¶åºå•ä½': time_info['time_unit']
            }
            
            results.append(result)
        else:
            print(f"è­¦å‘Š: åœ¨CSVçš„Datasetåˆ—ä¸­æœªæ‰¾åˆ° {dataset_match_name} çš„åŒ¹é…é¡¹")
    
    return pd.DataFrame(results)

def parse_log_file(log_file: str) -> Dict[str, int]:
    """
    è§£ææ—¥å¿—æ–‡ä»¶ï¼Œæå–æ•°æ®åå’Œç»†èƒæ•°ç›®
    """
    log_info = {}
    
    with open(log_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ•°æ®åå’Œç»†èƒæ•°ç›®
    pattern = r'ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶: (.*?\.link\.h5ad)[\s\S]*?ç»†èƒæ•°é‡ \(obs\): ([\d,]+)'
    matches = re.findall(pattern, content)
    
    for dataset_name, cell_count_str in matches:
        cell_count = int(cell_count_str.replace(',', ''))
        log_info[dataset_name] = cell_count
    
    return log_info

def extract_time_info(dataset_name: str, log_file: str) -> Dict[str, Any]:
    """
    ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–æ—¶åºä¿¡æ¯
    """
    with open(log_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–ç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯å—
    dataset_pattern = rf'ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶: {re.escape(dataset_name)}[\s\S]*?(?=ğŸ“„ æ­£åœ¨è¯»å–æ–‡ä»¶:|$)'
    dataset_match = re.search(dataset_pattern, content)
    
    time_info = {
        'has_time': 'å¦',
        'time_points': 0,
        'time_values': '',
        'time_unit': ''
    }
    
    if dataset_match:
        dataset_content = dataset_match.group(0)
        
        # æŸ¥æ‰¾timeåˆ—çš„ä¿¡æ¯
        time_pattern = r"'time'[\s\S]*?å”¯ä¸€å€¼æ•°ç›®: (\d+)[\s\S]*?å”¯ä¸€å€¼: (\[.*?\])"
        time_match = re.search(time_pattern, dataset_content)
        
        if time_match:
            time_points = int(time_match.group(1))
            time_values_str = time_match.group(2)
            
            if time_points > 1:
                time_info['has_time'] = 'æ˜¯'
                time_info['time_points'] = time_points
                
                # æå–å¹¶æ’åºæ—¶é—´å€¼
                time_values = eval(time_values_str)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨
                sorted_time_values = sorted(time_values, key=lambda x: float(x) if isinstance(x, (int, float, str)) and str(x).replace('.', '').isdigit() else x)
                time_info['time_values'] = ', '.join(map(str, sorted_time_values))
                
                # æ¨æ–­æ—¶é—´å•ä½
                time_info['time_unit'] = infer_time_unit_from_log(dataset_content)
    
    return time_info

def infer_time_unit_from_log(dataset_content: str) -> str:
    """
    ä»æ—¥å¿—å†…å®¹ä¸­æ¨æ–­æ—¶é—´å•ä½
    """
    # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ—¶é—´å•ä½çš„åˆ—å
    time_related_columns = re.findall(r"'(.*?(?:time|day|hour|week|month|year|sample|point).*?)'", dataset_content, re.IGNORECASE)
    
    for col in time_related_columns:
        col_lower = col.lower()
        if 'day' in col_lower or any(re.search(r'\bD\d', col) for col in time_related_columns):
            return 'å¤©'
        elif 'hour' in col_lower or 'hr' in col_lower:
            return 'å°æ—¶'
        elif 'week' in col_lower:
            return 'å‘¨'
        elif 'month' in col_lower:
            return 'æœˆ'
        elif 'year' in col_lower:
            return 'å¹´'
        elif 'minute' in col_lower or 'min' in col_lower:
            return 'åˆ†é’Ÿ'
    
    # å¦‚æœä»åˆ—åæ— æ³•æ¨æ–­ï¼Œæ£€æŸ¥æ—¶é—´å€¼
    time_values_pattern = r"å”¯ä¸€å€¼: (\[.*?\])"
    time_values_match = re.search(time_values_pattern, dataset_content)
    if time_values_match:
        time_values_str = time_values_match.group(1)
        if any(unit in time_values_str.lower() for unit in ['day', 'd']):
            return 'å¤©'
        elif any(unit in time_values_str.lower() for unit in ['hour', 'h']):
            return 'å°æ—¶'
        elif any(unit in time_values_str.lower() for unit in ['week', 'w']):
            return 'å‘¨'
        elif any(unit in time_values_str.lower() for unit in ['month', 'm']):
            return 'æœˆ'
    
    return 'æœªçŸ¥'

def determine_health_status(tissue_source: str) -> str:
    """
    æ ¹æ®ç»„ç»‡æ¥æºåˆ¤æ–­å¥åº·/ç–¾ç—…çŠ¶æ€
    """
    if 'Tumor' in tissue_source:
        return 'ç–¾ç—…'
    elif 'Organoid' in tissue_source:
        return 'ä½“å¤–æ¨¡å‹(ç±»å™¨å®˜)'
    elif 'Cell Line' in tissue_source:
        return 'ä½“å¤–æ¨¡å‹(ç»†èƒç³»)'
    elif 'Bone Marrow' in tissue_source or 'Hematopoietic' in tissue_source:
        return 'å¥åº·'
    else:
        return 'å¾…ç¡®è®¤'

def determine_perturbation(dataset_name: str) -> str:
    """
    æ ¹æ®æ•°æ®é›†åç§°åˆ¤æ–­æ˜¯å¦æ‰°åŠ¨
    """
    perturbation_keywords = ['pertur', 'treatment', 'drug', '5FU', 'TRAIL', 'RAS']
    if any(keyword.lower() in dataset_name.lower() for keyword in perturbation_keywords):
        return 'æ˜¯'
    else:
        return 'å¦'

def determine_perturbation_type(dataset_name: str) -> str:
    """
    æ ¹æ®æ•°æ®é›†åç§°åˆ¤æ–­æ‰°åŠ¨ç±»å‹
    """
    dataset_lower = dataset_name.lower()
    if '5fu' in dataset_lower:
        return 'åŒ–ç–—è¯ç‰©'
    elif 'trail' in dataset_lower:
        return 'å‡‹äº¡è¯±å¯¼'
    elif 'ras' in dataset_lower:
        return 'åŸºå› çªå˜'
    elif 'pertur' in dataset_lower:
        return 'æ‰°åŠ¨å®éªŒ'
    else:
        return 'æ— '

def main():
    """
    ä¸»å‡½æ•°
    """
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–è·¯å¾„å’Œæ¥æº
    if len(sys.argv) < 4:
        print("ç”¨æ³•: python script.py <CSVæ–‡ä»¶è·¯å¾„> <æ—¥å¿—æ–‡ä»¶è·¯å¾„> <å­˜å‚¨è·¯å¾„> <æ•°æ®æ¥æº>")
        print("ç¤ºä¾‹: python script.py scLTdb_Homo.csv view_h5ad.log.txt /path/to/data scLTdb")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    log_file = sys.argv[2]
    storage_path = sys.argv[3]
    data_source = sys.argv[4] if len(sys.argv) > 4 else "scLTdb"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_file):
        print(f"é”™è¯¯: CSVæ–‡ä»¶ {csv_file} ä¸å­˜åœ¨")
        sys.exit(1)
    
    if not os.path.exists(log_file):
        print(f"é”™è¯¯: æ—¥å¿—æ–‡ä»¶ {log_file} ä¸å­˜åœ¨")
        sys.exit(1)
    
    try:
        # æå–ä¿¡æ¯
        df = extract_dataset_info(csv_file, log_file, storage_path, data_source)
        
        if df.empty:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®é›†ä¿¡æ¯")
            return
        
        # è¾“å‡ºç»“æœ
        print("\næå–çš„æ•°æ®é›†ä¿¡æ¯:")
        print("=" * 120)
        print(df.to_string(index=False))
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = "dataset_info_extracted.csv"
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nç»Ÿè®¡ä¿¡æ¯:")
        print(f"å¤„ç†çš„æ•°æ®é›†æ•°é‡: {len(df)}")
        print(f"æ€»ç»†èƒæ•°ç›®: {df['ç»†èƒæ•°ç›®'].sum():,}")
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
